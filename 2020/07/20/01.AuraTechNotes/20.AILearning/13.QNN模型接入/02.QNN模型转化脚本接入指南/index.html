<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Linux系统上常用软件集锦 | 麦溪·在路上</title><meta name="author" content="Frewen.Wang"><meta name="copyright" content="Frewen.Wang"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Activity的生命周期完全解析"><link rel="shortcut icon" href="/img/frewen_tech.png"><link rel="canonical" href="http://www.frewen.wang/2020/07/20/01.AuraTechNotes/20.AILearning/13.QNN%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%85%A5/02.QNN%E6%A8%A1%E5%9E%8B%E8%BD%AC%E5%8C%96%E8%84%9A%E6%9C%AC%E6%8E%A5%E5%85%A5%E6%8C%87%E5%8D%97/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script async="async" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script>(adsbygoogle = window.adsbygoogle || []).push({
  google_ad_client: '',
  enable_page_level_ads: 'true'
});</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Linux系统上常用软件集锦',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-01-05 00:00:00'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/frewen_tech.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">864</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">158</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 文章归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 文章标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 文章归类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 文章列表</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/page_img.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="麦溪·在路上"><span class="site-name">麦溪·在路上</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 文章归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 文章标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 文章归类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 文章列表</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Linux系统上常用软件集锦</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-07-19T16:00:00.000Z" title="发表于 2020-07-20 00:00:00">2020-07-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-01-04T16:00:00.000Z" title="更新于 2022-01-05 00:00:00">2022-01-05</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">2.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>14分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Linux系统上常用软件集锦"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2020/07/20/01.AuraTechNotes/20.AILearning/13.QNN%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%85%A5/02.QNN%E6%A8%A1%E5%9E%8B%E8%BD%AC%E5%8C%96%E8%84%9A%E6%9C%AC%E6%8E%A5%E5%85%A5%E6%8C%87%E5%8D%97/#post-comment" itemprop="discussionUrl"><span class="valine-comment-count" data-xid="/2020/07/20/01.AuraTechNotes/20.AILearning/13.QNN%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%85%A5/02.QNN%E6%A8%A1%E5%9E%8B%E8%BD%AC%E5%8C%96%E8%84%9A%E6%9C%AC%E6%8E%A5%E5%85%A5%E6%8C%87%E5%8D%97/" itemprop="commentCount"><i class="fa-solid fa-spinner fa-spin"></i></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>[TOC]</p>
<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>QNN SDK提供了几个后端库。这些库可以在<QNN_SDK_ROOT>&#x2F;target&#x2F;<target-platform>&#x2F;lib文件夹中找到。QNN后端库名称以libQnn作为前缀。</p>
<img src="images/image-20220316144018709.png" alt="image-20220316144018709" style="zoom: 25%;" />





<p>本节包含与DSP后端API专门化相关的信息。所有QNN DSP后端专门化都可以在<QNN_SDK_ROOT>&#x2F;include&#x2F;DSP&#x2F;目录下使用。</p>
<img src="images/image-20220316144705397.png" alt="image-20220316144705397" style="zoom: 33%;" />



<h1 id="QNN量化"><a href="#QNN量化" class="headerlink" title="QNN量化"></a>QNN量化</h1><h2 id="qnn-onnx-converter"><a href="#qnn-onnx-converter" class="headerlink" title="qnn-onnx-converter"></a>qnn-onnx-converter</h2><p>进行ONNX模型转换成QNN模型，并进行量化的过程</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">--disable_batchnorm_folding <span class="comment"># 该参数为禁止优化 batchnorm 网络结构</span></span></span><br><span class="line">echo -e &quot;\n&gt;&gt;&gt;&gt; Step 2 : 执行 qnn-onnx-converter&quot;</span><br><span class="line"><span class="meta prompt_">$</span><span class="language-bash">&#123;QNN_SDK_BIN&#125;/qnn-onnx-converter \</span></span><br><span class="line"><span class="language-bash">    --input_network <span class="variable">$&#123;MODEL_DIR&#125;</span>/<span class="variable">$&#123;MODEL_ONNX&#125;</span> \</span></span><br><span class="line"><span class="language-bash">    --input_list <span class="variable">$&#123;INPUT_LIST_QUAN&#125;</span> \</span></span><br><span class="line"><span class="language-bash">    -o <span class="variable">$&#123;MODEL_QNN_DIR&#125;</span>/int8/cpp/<span class="variable">$&#123;MODEL_NAME&#125;</span>.cpp \</span></span><br><span class="line"><span class="language-bash">    --disable_batchnorm_folding</span></span><br></pre></td></tr></table></figure>

<p>脚本说明：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br></pre></td><td class="code"><pre><span class="line">usage: qnn-onnx-converter [--out_node OUT_NAMES]</span><br><span class="line">                          [--input_type INPUT_NAME INPUT_TYPE]</span><br><span class="line">                          [--input_dtype INPUT_NAME INPUT_DTYPE]</span><br><span class="line">                          [--input_encoding INPUT_NAME INPUT_ENCODING]</span><br><span class="line">                          [--input_layout INPUT_NAME INPUT_LAYOUT]</span><br><span class="line">                          [--debug [DEBUG]] [--dry_run [DRY_RUN]]</span><br><span class="line">                          [-d INPUT_NAME INPUT_DIM]</span><br><span class="line">                          [--batch BATCH_DIM]</span><br><span class="line">                          [--define_symbol SYMBOL_NAME VALUE]</span><br><span class="line">                          [--disable_batchnorm_folding]</span><br><span class="line">                          [--quantization_overrides QUANTIZATION_OVERRIDES]</span><br><span class="line">                          [--keep_quant_nodes]</span><br><span class="line">                          [--keep_disconnected_nodes]</span><br><span class="line">                          [--input_list INPUT_LIST]</span><br><span class="line">                          [--param_quantizer PARAM_QUANTIZER]</span><br><span class="line">                          [--act_quantizer ACT_QUANTIZER]</span><br><span class="line">                          [--algorithms ALGORITHMS [ALGORITHMS ...]]</span><br><span class="line">                          [--bias_bw BIAS_BW] [--act_bw ACT_BW]</span><br><span class="line">                          [--weight_bw WEIGHT_BW] [--ignore_encodings]</span><br><span class="line">                          [--use_per_channel_quantization [USE_PER_CHANNEL_QUANTIZATION [USE_PER_CHANNEL_QUANTIZATION ...]]]</span><br><span class="line">                          --input_network INPUT_NETWORK </span><br><span class="line">                          [-h] </span><br><span class="line">                          [-o OUTPUT_PATH   设置输出模型的路径]   </span><br><span class="line">                          [--copyright_file COPYRIGHT_FILE]</span><br><span class="line">                          [--overwrite_model_prefix] [--exclude_named_tensors]</span><br><span class="line">                          [--op_package_lib OP_PACKAGE_LIB]</span><br><span class="line">                          [-p PACKAGE_NAME | --op_package_config OP_PACKAGE_CONFIG [OP_PACKAGE_CONFIG ...]]</span><br><span class="line"></span><br><span class="line">Script to convert ONNX model into QNN</span><br><span class="line"></span><br><span class="line">required arguments:</span><br><span class="line">  --input_network INPUT_NETWORK, -i INPUT_NETWORK</span><br><span class="line">                        Path to the source framework model.</span><br><span class="line"></span><br><span class="line">optional arguments:</span><br><span class="line">  --out_node OUT_NODE   Name of the graph&#x27;s output nodes. Multiple output</span><br><span class="line">                        nodes should be provided separately like: --out_node</span><br><span class="line">                        out_1 --out_node out_2</span><br><span class="line">  --input_type INPUT_NAME INPUT_TYPE, -t INPUT_NAME INPUT_TYPE</span><br><span class="line">                        Type of data expected by each input op/layer. Type for</span><br><span class="line">                        each input is |default| if not specified. For example:</span><br><span class="line">                        &quot;data&quot; image.Note that the quotes should always be</span><br><span class="line">                        included in order to handle special characters,</span><br><span class="line">                        spaces,etc. For multiple inputs specify multiple</span><br><span class="line">                        --input_type on the command line. Eg: --input_type</span><br><span class="line">                        &quot;data1&quot; image --input_type &quot;data2&quot; opaque These</span><br><span class="line">                        options get used by DSP runtime and following</span><br><span class="line">                        descriptions state how input will be handled for each</span><br><span class="line">                        option. Image: Input is float between 0-255 and the</span><br><span class="line">                        input&#x27;s mean is 0.0f and the input&#x27;s max is 255.0f. We</span><br><span class="line">                        will cast the float to uint8ts and pass the uint8ts to</span><br><span class="line">                        the DSP. Default: Pass the input as floats to the dsp</span><br><span class="line">                        directly and the DSP will quantize it. Opaque: Assumes</span><br><span class="line">                        input is float because the consumer layer(i.e next</span><br><span class="line">                        layer) requires it as float, therefore it won&#x27;t be</span><br><span class="line">                        quantized. Choices supported: image default opaque</span><br><span class="line">   --input_dtype INPUT_NAME INPUT_DTYPE</span><br><span class="line">                        The names and datatype of the network input layers</span><br><span class="line">                        specified in the format [input_name datatype], for</span><br><span class="line">                        example: &#x27;data&#x27; &#x27;float32&#x27;. Default is float32 if not</span><br><span class="line">                        specified. Note that the quotes should always be</span><br><span class="line">                        included in order to handle special characters, spaces,</span><br><span class="line">                        etc. For multiple inputs specify multiple</span><br><span class="line">                        --input_dtype on the command line like: --input_dtype</span><br><span class="line">                        &#x27;data1&#x27; &#x27;float32&#x27; --input_dtype &#x27;data2&#x27; &#x27;float32&#x27;</span><br><span class="line">  --input_encoding INPUT_NAME INPUT_ENCODING, -e INPUT_NAME INPUT_ENCODING</span><br><span class="line">                        Image encoding of the source images. Default is bgr.</span><br><span class="line">                        Eg usage: &quot;data&quot; rgba Note the quotes should always be</span><br><span class="line">                        included in order to handle special characters,</span><br><span class="line">                        spaces, etc. For multiple inputs specify</span><br><span class="line">                        --input_encoding for each on the command line. Eg:</span><br><span class="line">                        --input_encoding &quot;data1&quot; rgba --input_encoding &quot;data2&quot;</span><br><span class="line">                        other Use options: color encodings(bgr,rgb, nv21...)</span><br><span class="line">                        if input is image; time_series: for inputs of rnn</span><br><span class="line">                        models; other: if input doesn&#x27;t follow above</span><br><span class="line">                        categories or is unknown. Choices supported: bgr rgb</span><br><span class="line">                        rgba argb32 nv21 time_series other</span><br><span class="line">  --debug [DEBUG]       Run the converter in debug mode.</span><br><span class="line">  --input_layout INPUT_NAME INPUT_LAYOUT, -l INPUT_NAME INPUT_LAYOUT</span><br><span class="line">                        Layout of each input tensor. If not specified, it will use the default</span><br><span class="line">                        based on the Source Framework, shape of input and input encoding.</span><br><span class="line">                        Accepted values are-</span><br><span class="line">                           NCDHW, NDHWC, NCHW, NHWC, NFC, NCF, NTF, TNF, NF, NC, F, NONTRIVIAL</span><br><span class="line">                        N = Batch, C = Channels, D = Depth, H = Height, W = Width, F = Feature, T = Time</span><br><span class="line">                        NDHWC/NCDHW used for 5d inputs</span><br><span class="line">                        NHWC/NCHW used for 4d image-like inputs</span><br><span class="line">                        NFC/NCF used for inputs to Conv1D or other 1D ops</span><br><span class="line">                        NTF/TNF used for inputs with time steps like the ones used for LSTM op</span><br><span class="line">                        NF used for 2D inputs, like the inputs to Dense/FullyConnected layers</span><br><span class="line">                        NC used for 2D inputs with 1 for batch and other for Channels (rarely used)</span><br><span class="line">                        F used for 1D inputs, e.g. Bias tensor</span><br><span class="line">                        NONTRIVIAL for everything elseFor multiple inputs specify multiple</span><br><span class="line">                        --input_layout on the command line.</span><br><span class="line">                        Eg:</span><br><span class="line">                           --input_layout &quot;data1&quot; NCHW --input_layout &quot;data2&quot; NCHW</span><br><span class="line">  --dry_run [DRY_RUN]   Evaluates the model without actually converting any</span><br><span class="line">                        ops, and returns unsupported ops/attributes as well as</span><br><span class="line">                        unused inputs and/or outputs if any. Leave empty or</span><br><span class="line">                        specify &quot;info&quot; to see dry run as a table, or specify</span><br><span class="line">                        &quot;debug&quot; to show more detailed messages only&quot;</span><br><span class="line">  -d INPUT_NAME INPUT_DIM, --input_dim INPUT_NAME INPUT_DIM</span><br><span class="line">                        The name and dimension of all the input buffers to the</span><br><span class="line">                        network specified in the format [input_name comma-</span><br><span class="line">                        separated-dimensions], for example: &#x27;data&#x27;</span><br><span class="line">                        1,224,224,3. Note that the quotes should always be</span><br><span class="line">                        included in order to handle special characters,</span><br><span class="line">                        spaces, etc. NOTE: This feature works only with Onnx</span><br><span class="line">                        1.6.0 and above</span><br><span class="line">  -n, --no_simplification</span><br><span class="line">                        Do not attempt to simplify the model automatically.</span><br><span class="line">                        This may prevent some models from properly converting</span><br><span class="line">                        when sequences of unsupported static operations are</span><br><span class="line">                        present.</span><br><span class="line">   -b BATCH, --batch BATCH</span><br><span class="line">                        The batch dimension override. This will take the first dimension of all</span><br><span class="line">                        inputs and treat it as a batch dim, overriding it with the value provided</span><br><span class="line">                        here. For example:</span><br><span class="line">                        --batch 6</span><br><span class="line">                        will result in a shape change from [1,3,224,224] to [6,3,224,224].</span><br><span class="line">                        If there are inputs without batch dim this should not be used and each input</span><br><span class="line">                        should be overridden independently using -d option for input dimension</span><br><span class="line">                        overrides.</span><br><span class="line">   -s SYMBOL_NAME VALUE, --define_symbol SYMBOL_NAME VALUE</span><br><span class="line">                        This option allows overriding specific input dimension symbols. For instance</span><br><span class="line">                        you might see input shapes specified with variables such as :</span><br><span class="line">                        data: [1,3,height,width]</span><br><span class="line">                        To override these simply pass the option as:</span><br><span class="line">                        --define_symbol height 224 --define_symbol width 448</span><br><span class="line">                        which results in dimensions that look like:</span><br><span class="line">                        data: [1,3,224,448]</span><br><span class="line">  --disable_batchnorm_folding</span><br><span class="line">                        If not specified, converter will try to fold batchnorm into previous layer.</span><br><span class="line">  --keep_disconnected_nodes</span><br><span class="line">                        Disable Optimization that removes Ops not connected to the main graph.</span><br><span class="line">                        This optimization uses output names provided over commandline OR</span><br><span class="line">                        inputs/outputs extracted from the Source model to determine the main graph</span><br><span class="line">  -h, --help            show this help message and exit</span><br><span class="line">  -o OUTPUT_PATH, --output_path OUTPUT_PATH</span><br><span class="line">                        Path where the converted Output model should be</span><br><span class="line">                        saved.If not specified, the converter model will be</span><br><span class="line">                        written to a file with same name as the input model</span><br><span class="line">  --copyright_file COPYRIGHT_FILE</span><br><span class="line">                        Path to copyright file. If provided, the content of</span><br><span class="line">                        the file will be added to the output model.</span><br><span class="line">  --overwrite_model_prefix</span><br><span class="line">                        If option passed, model generator will use the output</span><br><span class="line">                        path name to use as model prefix to name functions in</span><br><span class="line">                        &lt;qnn_model_name&gt;.cpp. (Useful for running multiple</span><br><span class="line">                        models at once) eg: ModelName_composeGraphs. Default</span><br><span class="line">                        is to use generic &quot;QnnModel_&quot;.</span><br><span class="line">  --exclude_named_tensors</span><br><span class="line">                        Remove using source framework tensorNames; instead use</span><br><span class="line">                        a counter for naming tensors. Note: This can</span><br><span class="line">                        potentially help to reduce the final model library</span><br><span class="line">                        that will be generated(Recommended for deploying</span><br><span class="line">                        model). Default is False.</span><br><span class="line"></span><br><span class="line">Quantizer Options:</span><br><span class="line">  --quantization_overrides QUANTIZATION_OVERRIDES</span><br><span class="line">                        Use this option to specify a json file with parameters</span><br><span class="line">                        to use for quantization. These will override any</span><br><span class="line">                        quantization data carried from conversion (eg TF fake</span><br><span class="line">                        quantization) or calculated during the normal</span><br><span class="line">                        quantization process. Format defined as per AIMET</span><br><span class="line">                        specification.</span><br><span class="line">  --keep_quant_nodes    Use this option to keep activation quantization nodes</span><br><span class="line">                        in the graph rather than stripping them.</span><br><span class="line">  --input_list INPUT_LIST</span><br><span class="line">                        Path to a file specifying the input data. This file</span><br><span class="line">                        should be a plain text file, containing one or more</span><br><span class="line">                        absolute file paths per line. Each path is expected to</span><br><span class="line">                        point to a binary file containing one input in the</span><br><span class="line">                        &quot;raw&quot; format, ready to be consumed by the quantizer</span><br><span class="line">                        without any further preprocessing. Multiple files per</span><br><span class="line">                        line separated by spaces indicate multiple inputs to</span><br><span class="line">                        the network. See documentation for more details. Must</span><br><span class="line">                        be specified for quantization. All subsequent</span><br><span class="line">                        quantization options are ignored when this is not</span><br><span class="line">                        provided.</span><br><span class="line">  --param_quantizer PARAM_QUANTIZER</span><br><span class="line">                        Optional parameter to indicate the weight/bias</span><br><span class="line">                        quantizer to use. Must be followed by one of the</span><br><span class="line">                        following options: &quot;tf&quot;: Uses the real min/max of the</span><br><span class="line">                        data and specified bitwidth (default) &quot;enhanced&quot;: Uses</span><br><span class="line">                        an algorithm useful for quantizing models with long</span><br><span class="line">                        tails present in the weight distribution &quot;adjusted&quot;:</span><br><span class="line">                        Uses an adjusted min/max for computing the range,</span><br><span class="line">                        particularly good for denoise models &quot;symmetric&quot;:</span><br><span class="line">                        Ensures min and max have the same absolute values</span><br><span class="line">                        about zero. Data will be stored as int#_t data such</span><br><span class="line">                        that the offset is always 0.</span><br><span class="line">  --act_quantizer ACT_QUANTIZER</span><br><span class="line">                        Optional parameter to indicate the activation</span><br><span class="line">                        quantizer to use. Must be followed by one of the</span><br><span class="line">                        following options: &quot;tf&quot;: Uses the real min/max of the</span><br><span class="line">                        data and specified bitwidth (default) &quot;enhanced&quot;: Uses</span><br><span class="line">                        an algorithm useful for quantizing models with long</span><br><span class="line">                        tails present in the weight distribution &quot;adjusted&quot;:</span><br><span class="line">                        Uses an adjusted min/max for computing the range,</span><br><span class="line">                        particularly good for denoise models &quot;symmetric&quot;:</span><br><span class="line">                        Ensures min and max have the same absolute values</span><br><span class="line">                        about zero. Data will be stored as int#_t data such</span><br><span class="line">                        that the offset is always 0.</span><br><span class="line">  --algorithms ALGORITHMS [ALGORITHMS ...]</span><br><span class="line">                        Use this option to enable new optimization algorithms.</span><br><span class="line">                        Usage is: --algorithms &lt;algo_name1&gt; ... The available</span><br><span class="line">                        optimization algorithms are: &quot;cle&quot; - Cross layer</span><br><span class="line">                        equalization includes a number of methods for</span><br><span class="line">                        equalizing weights and biases across layers in order</span><br><span class="line">                        to rectify imbalances that cause quantization errors.</span><br><span class="line">                        &quot;bc&quot; - Bias correction adjusts biases to offset</span><br><span class="line">                        activation quantization errors. Typically used in</span><br><span class="line">                        conjunction with &quot;cle&quot; to improve quantization</span><br><span class="line">                        accuracy.</span><br><span class="line">  --bias_bw BIAS_BW     Use the --bias_bw option to select the bitwidth to use</span><br><span class="line">                        when quantizing the biases, either 8 (default) or 32.</span><br><span class="line">  --act_bw ACT_BW       Use the --act_bw option to select the bitwidth to use</span><br><span class="line">                        when quantizing the activations, either 8 (default) or</span><br><span class="line">                        16.</span><br><span class="line">  --weight_bw WEIGHT_BW</span><br><span class="line">                        Use the --weight_bw option to select the bitwidth to</span><br><span class="line">                        use when quantizing the weights, currently only 8 bit</span><br><span class="line">                        (default) supported.</span><br><span class="line">  --ignore_encodings    Use only quantizer generated encodings, ignoring any</span><br><span class="line">                        user or model provided encodings. Note: Cannot use</span><br><span class="line">                        --ignore_encodings with --quantization_overrides</span><br><span class="line">  --use_per_channel_quantization [USE_PER_CHANNEL_QUANTIZATION [USE_PER_CHANNEL_QUANTIZATION ...]]</span><br><span class="line">                        Use per-channel quantization for convolution-based op</span><br><span class="line">                        weights. Note: This will replace built-in model QAT</span><br><span class="line">                        encodings when used for a given weight.Usage &quot;--</span><br><span class="line">                        use_per_channel_quantization&quot; to enable or &quot;--</span><br><span class="line">                        use_per_channel_quantization false&quot; (default) to</span><br><span class="line">                        disable</span><br><span class="line"></span><br><span class="line">Custom Op Package Options:</span><br><span class="line">  --op_package_lib OP_PACKAGE_LIB, -opl OP_PACKAGE_LIB</span><br><span class="line">                        Use this argument to pass an op package library for</span><br><span class="line">                        quantization. Must be in the form</span><br><span class="line">                        &lt;op_package_lib_path:interfaceProviderName&gt; and be</span><br><span class="line">                        separated by a comma for multiple package libs</span><br><span class="line">  -p PACKAGE_NAME, --package_name PACKAGE_NAME</span><br><span class="line">                        A global package name to be used for each node in the</span><br><span class="line">                        Model.cpp file. Defaults to Qnn header defined package</span><br><span class="line">                        name</span><br><span class="line">  --op_package_config OP_PACKAGE_CONFIG [OP_PACKAGE_CONFIG ...], -opc OP_PACKAGE_CONFIG [OP_PACKAGE_CONFIG ...]</span><br><span class="line">                        Path to a Qnn Op Package XML configuration file that</span><br><span class="line">                        contains user defined custom operations.</span><br><span class="line"></span><br><span class="line">Note: Only one of: &#123;&#x27;package_name&#x27;, &#x27;op_package_config&#x27;&#125; can be specified</span><br></pre></td></tr></table></figure>





<h2 id="qnn-model-lib-generator"><a href="#qnn-model-lib-generator" class="headerlink" title="qnn-model-lib-generator"></a>qnn-model-lib-generator</h2><p>qnn-model-lib-generator 工具将 QNN 模型源代码编译为特定目标生成物。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">echo -e &quot;\n&gt;&gt;&gt;&gt; Step 3 : 执行 qnn-model-lib-generator&quot;</span><br><span class="line"><span class="meta prompt_">$</span><span class="language-bash">&#123;QNN_SDK_BIN&#125;/qnn-model-lib-generator \</span></span><br><span class="line"><span class="language-bash">     -c <span class="variable">$&#123;MODEL_QNN_DIR&#125;</span>/int8/cpp/<span class="variable">$&#123;MODEL_NAME&#125;</span>.cpp \</span></span><br><span class="line"><span class="language-bash">     -b <span class="variable">$&#123;MODEL_QNN_DIR&#125;</span>/int8/cpp/<span class="variable">$&#123;MODEL_NAME&#125;</span>.bin \</span></span><br><span class="line"><span class="language-bash">     -o <span class="variable">$&#123;MODEL_QNN_DIR&#125;</span>/int8/so/ \</span></span><br><span class="line"><span class="language-bash">     -t x86_64-linux-clang</span></span><br></pre></td></tr></table></figure>

<p>命令行注释：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">usage: qnn-model-lib-generator</span><br><span class="line">		[-h]</span><br><span class="line">        [-c &lt;QNN_MODEL&gt;.cpp   QNN模型上一步生成的cpp文件] </span><br><span class="line">        [-b &lt;QNN_MODEL&gt;.bin	   QNN模型上一步生成的cpp文件]</span><br><span class="line">        [-t LIB_TARGETS 		生成QNN模型的目标类型：x86_64-linux-clang、aarch64-android、x86_64-linux-clang、arm-android] </span><br><span class="line">        [-l LIB_NAME				指定库的名称] 			</span><br><span class="line">        [-o OUTPUT_DIR         输出SO文件的目录]</span><br><span class="line">        </span><br><span class="line">Required argument(s):</span><br><span class="line"> -c &lt;QNN_MODEL&gt;.cpp                    Filepath for the qnn model .cpp file</span><br><span class="line"></span><br><span class="line">optional argument(s):</span><br><span class="line"> -b &lt;QNN_MODEL&gt;.bin                    Filepath for the qnn model .bin file</span><br><span class="line">                                       (Note: if not passed, runtime will fail if .cpp needs any items from a .bin file.)</span><br><span class="line"></span><br><span class="line"> -t LIB_TARGETS                        Specifies the targets to build the models for. Default: aarch64-android x86_64-linux-clang arm-android</span><br><span class="line"> -l LIB_NAME                           Specifies the name to use for libraries. Default: uses name in &lt;model.bin&gt; if provided,</span><br><span class="line">                                       else generic qnn_model.so</span><br><span class="line">  -o OUTPUT_DIR                         Location for saving output libraries.</span><br></pre></td></tr></table></figure>





<h2 id="qnn-context-binary-generator"><a href="#qnn-context-binary-generator" class="headerlink" title="qnn-context-binary-generator"></a>qnn-context-binary-generator</h2><p>qnn-context-binary-generator 工具用于通过使用从 QNN 转换器的输出和特定后端编译的模型库来创建上下文二进制文件。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">echo -e &quot;\n&gt;&gt;&gt;&gt; Step 4 : 执行 qnn-context-binary-generator&quot;</span><br><span class="line"><span class="meta prompt_">$</span><span class="language-bash">&#123;QNN_SDK_BIN&#125;/qnn-context-binary-generator \</span></span><br><span class="line"><span class="language-bash">     --backend <span class="variable">$&#123;QNN_SDK_BIN&#125;</span>/../lib/libQnnHtp.so \</span></span><br><span class="line"><span class="language-bash">     --model <span class="variable">$&#123;MODEL_QNN_DIR&#125;</span>/int8/so/x86_64-linux-clang/lib<span class="variable">$&#123;MODEL_NAME&#125;</span>.so \</span></span><br><span class="line"><span class="language-bash">     --binary_file <span class="variable">$&#123;MODEL_NAME&#125;</span>_int8 \</span></span><br><span class="line"><span class="language-bash">     --output_dir <span class="variable">$&#123;MODEL_QNN_DIR&#125;</span>/int8/so/x86_64-linux-clang</span></span><br></pre></td></tr></table></figure>



<p>命令行说明：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">usage: qnn-context-binary-generator --model QNN_MODEL.so --backend QNN_BACKEND.so</span><br><span class="line">                                    --binary_file BINARY_FILE_NAME</span><br><span class="line">                                    [--model_prefix MODEL_PREFIX]</span><br><span class="line">                                    [--output_dir OUTPUT_DIRECTORY]</span><br><span class="line">                                    [--op_packages ONE_OR_MORE_OP_PACKAGES]</span><br><span class="line">                                    [--config_file CONFIG_FILE.json]</span><br><span class="line">                                    [--verbose] [--version] [--help]</span><br><span class="line"></span><br><span class="line">REQUIRED ARGUMENTS:</span><br><span class="line">-------------------</span><br><span class="line">  --model             &lt;FILE&gt;      包含 QNN 网络的so文件的路径.</span><br><span class="line"></span><br><span class="line">  --backend           &lt;FILE&gt;      Path to a QNN backend .so library to create the context binary.</span><br><span class="line"></span><br><span class="line">  --binary_file       &lt;VAL&gt;      用于保存上下文二进制文件的二进制文件的名称。保存在与 --output_dir 选项相同的路径中，并以 .bin 作为二进制文件扩展名。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">OPTIONAL ARGUMENTS:</span><br><span class="line">-------------------</span><br><span class="line">  --model_prefix                  Function prefix to use when loading &lt;qnn_model_name.so&gt;file</span><br><span class="line">                                  containing a QNN network. Default: QnnModel.</span><br><span class="line"></span><br><span class="line">  --output_dir        &lt;DIR&gt;       The directory to save output to. Defaults to ./output.</span><br><span class="line"></span><br><span class="line">  --op_packages       &lt;VAL&gt;       Provide a comma separated list of op packages</span><br><span class="line">                                  and interface providers to register. The syntax is:</span><br><span class="line">                                  op_package_path:interface_provider[,op_package_path:interface_provider...]</span><br><span class="line"></span><br><span class="line">  --config_file       &lt;FILE&gt;      Path to a JSON config file. The config file currently</span><br><span class="line">                                  supports options related to backend extensions and</span><br><span class="line">                                  context priority. Please refer to SDK documentation</span><br><span class="line">                                  for more details.</span><br><span class="line"></span><br><span class="line">  --enable_intermediate_outputs   Enable all intermediate nodes to be output along with</span><br><span class="line">                                  default outputs in the saved context.</span><br><span class="line"></span><br><span class="line">  --log_level                     Specifies max logging level to be set. Valid settings:</span><br><span class="line">                                  &quot;error&quot;, &quot;warn&quot;, &quot;info&quot; and &quot;verbose&quot;.</span><br><span class="line"></span><br><span class="line">  --version                       Print the QNN SDK version.</span><br><span class="line"></span><br><span class="line">  --help                          Show this help message.</span><br></pre></td></tr></table></figure>



</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://www.frewen.wang">Frewen.Wang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://www.frewen.wang/2020/07/20/01.AuraTechNotes/20.AILearning/13.QNN%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%85%A5/02.QNN%E6%A8%A1%E5%9E%8B%E8%BD%AC%E5%8C%96%E8%84%9A%E6%9C%AC%E6%8E%A5%E5%85%A5%E6%8C%87%E5%8D%97/">http://www.frewen.wang/2020/07/20/01.AuraTechNotes/20.AILearning/13.QNN%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%85%A5/02.QNN%E6%A8%A1%E5%9E%8B%E8%BD%AC%E5%8C%96%E8%84%9A%E6%9C%AC%E6%8E%A5%E5%85%A5%E6%8C%87%E5%8D%97/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://www.frewen.wang" target="_blank">麦溪·在路上</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%9B%9B%E5%A4%A7%E7%BB%84%E4%BB%B6/">四大组件</a><a class="post-meta__tags" href="/tags/Activity/">Activity</a><a class="post-meta__tags" href="/tags/Linux/">Linux</a></div><div class="post_share"><div class="social-share" data-image="/img/frewen_tech.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat_square.jpg" target="_blank"><img class="post-qr-code-img" src="/img/wechat_square.jpg" alt="微信打赏"/></a><div class="post-qr-code-desc">微信打赏</div></li><li class="reward-item"><a href="/img/alipay_square.jpg" target="_blank"><img class="post-qr-code-img" src="/img/alipay_square.jpg" alt="支付宝打赏"/></a><div class="post-qr-code-desc">支付宝打赏</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/07/20/01.AuraTechNotes/20.AILearning/13.QNN%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%85%A5/02.QNN%E5%BC%80%E5%8F%91%E4%B9%8BQNN%E5%90%8E%E7%AB%AF%E5%AD%A6%E4%B9%A0/" title="Linux系统上常用软件集锦"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Linux系统上常用软件集锦</div></div></a></div><div class="next-post pull-right"><a href="/2020/07/20/01.AuraTechNotes/20.AILearning/13.QNN%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%85%A5/03.QNN%E5%BC%80%E5%8F%91%E4%B9%8BQNN%E8%B0%83%E8%AF%95%E5%B7%A5%E5%85%B7/" title="QNN开发之QNN调试工具"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">QNN开发之QNN调试工具</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2020/07/20/01.AuraTechNotes/14.WebLearn/05.UniApp%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/01.UniApp%E7%9A%84%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" title="UniApp的环境搭建"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-07-20</div><div class="title">UniApp的环境搭建</div></div></a></div><div><a href="/2020/07/20/01.AuraTechNotes/16.Linux/04.Linux%E4%B9%8Bvim%E7%BC%96%E8%BE%91%E5%99%A8/01.Linux%E4%B9%8Bvim%E7%BC%96%E8%BE%91%E5%99%A8%E4%BD%BF%E7%94%A8%E5%9F%BA%E7%A1%80/" title="Linux系统上常用软件集锦"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-07-20</div><div class="title">Linux系统上常用软件集锦</div></div></a></div><div><a href="/2020/07/20/01.AuraTechNotes/16.Linux/01.Linux%E4%B9%8B%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85/01.Linux%E7%B3%BB%E7%BB%9F%E4%B8%8A%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6%E9%9B%86%E9%94%A6/" title="Linux系统上常用软件集锦"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-07-20</div><div class="title">Linux系统上常用软件集锦</div></div></a></div><div><a href="/2020/07/20/01.AuraTechNotes/16.Linux/01.Linux%E4%B9%8B%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85/02.Linux%E7%B3%BB%E7%BB%9F%E7%9A%84%E7%9B%AE%E5%BD%95%E8%AF%B4%E6%98%8E/" title="Linux系统上常用软件集锦"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-07-20</div><div class="title">Linux系统上常用软件集锦</div></div></a></div><div><a href="/2020/07/20/01.AuraTechNotes/16.Linux/01.Linux%E4%B9%8B%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85/03.Windows%E5%92%8CLinux%E5%8F%8C%E7%B3%BB%E7%BB%9F%E7%A1%AC%E7%9B%98%E6%89%A9%E5%AE%B9/" title="Linux系统上常用软件集锦"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-07-20</div><div class="title">Linux系统上常用软件集锦</div></div></a></div><div><a href="/2020/07/20/01.AuraTechNotes/16.Linux/01.Linux%E4%B9%8B%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85/06.MacOS%E7%94%A8Parallels%20Desktop%E5%AE%89%E8%A3%85Ubuntu/" title="Linux系统上常用软件集锦"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-07-20</div><div class="title">Linux系统上常用软件集锦</div></div></a></div></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div class="comment-switch"><span class="first-comment">Valine</span><span id="switch-btn"></span><span class="second-comment">Disqus</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div><div><div id="disqus_thread"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/frewen_tech.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Frewen.Wang</div><div class="author-info__description">在青麦地上跑着,雪和太阳的光芒</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">864</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">158</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/FrewenWang"><i class="fab fa-github"></i><span>关注我</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/FrewenWang" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:frewen1225@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://plus.google.com/FrewenWong" target="_blank" title="Google"><i class="fa-brands fa-google"></i></a><a class="social-icon" href="https://twitter.com/FrewenWong" target="_blank" title="Twitter"><i class="fa-brands fa-twitter"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0"><span class="toc-number">1.</span> <span class="toc-text">概述</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#QNN%E9%87%8F%E5%8C%96"><span class="toc-number">2.</span> <span class="toc-text">QNN量化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#qnn-onnx-converter"><span class="toc-number">2.1.</span> <span class="toc-text">qnn-onnx-converter</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#qnn-model-lib-generator"><span class="toc-number">2.2.</span> <span class="toc-text">qnn-model-lib-generator</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#qnn-context-binary-generator"><span class="toc-number">2.3.</span> <span class="toc-text">qnn-context-binary-generator</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/12/22/01.AuraTechNotes/16.Linux/03.Linux%E4%B9%8Bshell%E8%84%9A%E6%9C%AC%E5%AD%A6%E4%B9%A0/18.shell%E8%84%9A%E6%9C%AC%E5%B8%B8%E7%94%A8%E5%AE%9E%E4%BE%8B%E5%A4%87%E4%BB%BD/" title="无题">无题</a><time datetime="2023-12-22T13:48:40.121Z" title="发表于 2023-12-22 21:48:40">2023-12-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/07/28/01.AuraTechNotes/09.Python/12.python%E7%9A%84pycharm%E7%9A%84%E4%BD%BF%E7%94%A8/01.pycharm%E7%9A%84%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/" title="无题">无题</a><time datetime="2023-07-28T11:06:28.000Z" title="发表于 2023-07-28 19:06:28">2023-07-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/07/27/01.AuraTechNotes/09.Python/11.python%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/01.python%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/" title="无题">无题</a><time datetime="2023-07-27T02:02:00.000Z" title="发表于 2023-07-27 10:02:00">2023-07-27</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/07/20/01.AuraTechNotes/16.Linux/01.Linux%E4%B9%8B%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85/01.%E5%9F%BA%E4%BA%8EWindows11%E5%AE%89%E8%A3%85Ubuntu%E5%8F%8C%E7%B3%BB%E7%BB%9F/" title="基于Windows11安装Ubuntu双系统">基于Windows11安装Ubuntu双系统</a><time datetime="2023-07-19T16:00:00.000Z" title="发表于 2023-07-20 00:00:00">2023-07-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/07/17/01.AuraTechNotes/09.Python/01.python%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E5%AD%A6%E4%B9%A0/12.%E6%90%AD%E5%BB%BAbaidu%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/" title="无题">无题</a><time datetime="2023-07-17T12:35:38.000Z" title="发表于 2023-07-17 20:35:38">2023-07-17</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By Frewen.Wang</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="chat-btn" type="button" title="聊天"><i class="fas fa-sms"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>(() => {
  const initValine = () => {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: '',
      appKey: '',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  const loadValine = async () => {
    if (typeof Valine === 'function') initValine()
    else {
      await getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js')
      initValine()
    }
  }

  if ('Valine' === 'Valine' || !false) {
    if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
    else setTimeout(loadValine, 0)
  } else {
    window.loadOtherComment = loadValine
  }
})()</script><script>(() => {
  const disqus_config = function () {
    this.page.url = 'http://www.frewen.wang/2020/07/20/01.AuraTechNotes/20.AILearning/13.QNN%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%85%A5/02.QNN%E6%A8%A1%E5%9E%8B%E8%BD%AC%E5%8C%96%E8%84%9A%E6%9C%AC%E6%8E%A5%E5%85%A5%E6%8C%87%E5%8D%97/'
    this.page.identifier = '/2020/07/20/01.AuraTechNotes/20.AILearning/13.QNN%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%85%A5/02.QNN%E6%A8%A1%E5%9E%8B%E8%BD%AC%E5%8C%96%E8%84%9A%E6%9C%AC%E6%8E%A5%E5%85%A5%E6%8C%87%E5%8D%97/'
    this.page.title = 'Linux系统上常用软件集锦'
  }

  const disqusReset = () => {
    window.DISQUS && window.DISQUS.reset({
      reload: true,
      config: disqus_config
    })
  }

  btf.addGlobalFn('themeChange', disqusReset, 'disqus')

  const loadDisqus = () =>{
    if (window.DISQUS) disqusReset()
    else {
      const script = document.createElement('script')
      script.src = 'https://.disqus.com/embed.js'
      script.setAttribute('data-timestamp', +new Date())
      document.head.appendChild(script)
    }
  }

  const getCount = async() => {
    try {
      const eleGroup = document.querySelector('#post-meta .disqus-comment-count')
      if (!eleGroup) return
      const cleanedLinks = eleGroup.href.replace(/#post-comment$/, '')

      const res = await fetch(`https://disqus.com/api/3.0/threads/set.json?forum=&api_key=&thread:link=${cleanedLinks}`,{
        method: 'GET'
      })
      const result = await res.json()

      const count = result.response.length ? result.response[0].posts : 0
      eleGroup.textContent = count
    } catch (err) {
      console.error(err)
    }
  }

  if ('Valine' === 'Disqus' || !false) {
    if (false) btf.loadComment(document.getElementById('disqus_thread'), loadDisqus)
    else {
      loadDisqus()
      GLOBAL_CONFIG_SITE.isPost && getCount()
    }
  } else {
    window.loadOtherComment = loadDisqus
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>